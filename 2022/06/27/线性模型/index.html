<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="buptyqx">





<title>test2 | buptyqx&#39;s BLOG</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 5.4.2"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">BUPTyqx&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">BUPTyqx&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">test2</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">buptyqx</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">June 27, 2022&nbsp;&nbsp;14:40:02</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="机器学习笔记"><a href="#机器学习笔记" class="headerlink" title="机器学习笔记"></a>机器学习笔记</h1><h1 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h1><h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h2><h3 id="1-1求解偏置b"><a href="#1-1求解偏置b" class="headerlink" title="1.1求解偏置b"></a>1.1求解偏置b</h3><p>推导思路：</p>
<ul>
<li>==由最小二乘法导出损失函数$E(w,b)$==</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
E_{(w, b)} &=\sum_{i=1}^{m}\left(y_{i}-f\left(x_{i}\right)\right)^{2} \\
&=\sum_{i=1}^{m}\left(y_{i}-\left(w x_{i}+b\right)\right)^{2} \\
&=\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}
\end{aligned}</script><ul>
<li>==证明损失函数$E(w,b)$是关于$w$和$b$的凸函数==</li>
</ul>
<p>【判断凸凹函数的方法】</p>
<p><strong>二元函数判断凹凸性：</strong></p>
<p>设$f(x,y)$在区域D上具有二阶连续偏导数，记$A=f_{xx}^{‘’}(x,y),B=f_{xy}^{‘’}(x,y),C=f_{yy}^{‘’}(x,y)$，则：</p>
<p>(1)在$D$上恒有$A&gt;0$，且$AC-B^2\ge0$，$f(x,y)$在区域D上是凸函数</p>
<p>(2)在$D$上恒有$A&lt;0$，且$AC-B^2\ge 0$，$f(x,y)$在区域D上是凹函数</p>
<p><strong>二元凸凹函数求最值</strong></p>
<p>设$f(x,y)$是在开区域$D$内具有连续偏导数的凸函数，$(x_0,y_0)\in D$且$f_{x}^{\prime}\left(x_{0}, y_{0}\right)=0, f_{y}^{\prime}\left(x_{0}, y_{0}\right)=0$，则$f(x_0,y_0)$必定为$f(x,y)$在$D$内的最小值（最大值）</p>
<p><strong>证明损失函数$E(w,b)$是关于$w$和$b$的凸函数——求$A=f_{xx}^{‘’}(x,y),B=f_{xy}^{‘’}(x,y),C=f_{yy}^{‘’}(x,y)$</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_{(w, b)}}{\partial w} &=\frac{\partial}{\partial w}\left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\
&=\sum_{i=1}^{m} \frac{\partial}{\partial w}\left(y_{i}-w x_{i}-b\right)^{2} \\
&=\sum_{i=1}^{m} 2 \cdot\left(y_{i}-w x_{i}-b\right) \cdot\left(-x_{i}\right) \\
&=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial^{2} E_{(w, b)}}{\partial w^{2}} &=\frac{\partial}{\partial w}\left(\frac{\partial E_{(w, b)}}{\partial w}\right) \\
&=\frac{\partial}{\partial w}\left[2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)\right] \\
&=\frac{\partial}{\partial w}\left[2 w \sum_{i=1}^{m} x_{i}^{2}\right]
\\
&=2 \sum_{i=1}^{m} x_{i}^{2} \quad \text { 此即为 } A=f_{x x}^{\prime \prime}(x, y)
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\frac{\partial^{2} E_{(w, b)}}{\partial w \partial b} &=\frac{\partial}{\partial b}\left(\frac{\partial E_{(w, b)}}{\partial w}\right) \\
&=\frac{\partial}{\partial b}\left[2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)\right] \\
&=\frac{\partial}{\partial b}\left[-2 \sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right] \\
&=\frac{\partial}{\partial b}\left(-2 \sum_{i=1}^{m} y_{i} x_{i}+2 \sum_{i=1}^{m} b x_{i}\right)
\\
&=\frac{\partial}{\partial b}\left(2 \sum_{i=1}^{m} b x_{i}\right)=2 \sum_{i=1}^{m} x_{i} \quad \text { 此即为 } B=f_{x y}^{\prime \prime}(x, y)
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}

\frac{\partial E_{(w, b)}}{\partial b} &=\frac{\partial}{\partial b}\left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\
&=\sum_{i=1}^{m} \frac{\partial}{\partial b}\left(y_{i}-w x_{i}-b\right)^{2} \\
&=\sum_{i=1}^{m} 2 \cdot\left(y_{i}-w x_{i}-b\right) \cdot(-1)
\\
&=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right) \quad \text { 此即为式3.6 }
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial^{2} E_{(w, b)}}{\partial b^{2}} &=\frac{\partial}{\partial b}\left(\frac{\partial E_{(w, b)}}{\partial b}\right) \\
&=\frac{\partial}{\partial b}\left[2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)\right] \\
&=\frac{\partial}{\partial b}(2 m b) \\
&=2 m \quad \text { 此即为 } C=f_{y y}^{\prime \prime}(x, y)
\end{aligned}</script><script type="math/tex; mode=display">
A=2 \sum_{i=1}^{m} x_{i}^{2} \quad B=2 \sum_{i=1}^{m} x_{i} \quad C=2 m \\
A C-B^{2}=2 m \cdot 2 \sum_{i=1}^{m} x_{i}^{2}-\left(2 \sum_{i=1}^{m} x_{i}\right)^{2}=4 m \sum_{i=1}^{m} x_{i}^{2}-4\left(\sum_{i=1}^{m} x_{i}\right)^{2}
\\=4 m \sum_{i=1}^{m} x_{i}^{2}-4 \cdot m \cdot \frac{1}{m} \cdot\left(\sum_{i=1}^{m} x_{i}\right)^{2}
\\=4 m \sum_{i=1}^{m} x_{i}^{2}-4 m \cdot \bar{x} \cdot \sum_{i=1}^{m} x_{i}=4 m\left(\sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m} x_{i} \bar{x}\right)=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \bar{x}\right)
\\又因为\sum_{i=1}^{m} x_{i} \bar{x}=\bar{x} \sum_{i=1}^{m} x_{i}=\bar{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} x_{i}=m \bar{x}^{2}=\sum_{i=1}^{m} \bar{x}^{2}\\
=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \bar{x}-x_{i} \bar{x}+x_{i} \bar{x}\right)=4 m \sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \bar{x}-x_{i} \bar{x}+\bar{x}^{2}\right)=4 m \sum_{i=1}^{m}\left(x_{i}-\bar{x}\right)^{2}</script><script type="math/tex; mode=display">
A C-B^{2}=4 m \sum_{i=1}^{m}\left(x_{i}-\bar{x}\right)^{2} \geq 0</script><ul>
<li>==对损失函数$E(w,b)$关于b求一阶偏导数==</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)</script><ul>
<li>==令一阶偏导数等于0求出$b$==</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)=0 \\
m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)=0\\
b=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right) \quad \text { 此即为式3.8 }\\
b=\frac{1}{m} \sum_{i=1}^{m} y_{i}-w \cdot \frac{1}{m} \sum_{i=1}^{m} x_{i}=\bar{y}-w \bar{x}
\end{aligned}</script><h3 id="1-2求解权重w"><a href="#1-2求解权重w" class="headerlink" title="1.2求解权重w"></a>1.2求解权重w</h3><ul>
<li>==由最小二乘法推导出损失函数$E(w,b)$==</li>
<li>==证明损失函数$E(w,b)$是关于$e$和$b$的凸函数==</li>
<li>==对损失函数$E(w,b)$关于$w$求一阶偏导数==</li>
<li>==令一阶偏导数等于0求解$w$==</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_{(w, b)}}{\partial w}=& 2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)=0 \\
& w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}=0 \\
& w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m} b x_{i}
\end{aligned}</script><script type="math/tex; mode=display">
b=\bar{y}-w \bar{x} \text { 代人 } w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m} b x_{i} \text { 可得 } \\
w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\sum_{i=1}^{m}(\bar{y}-w \bar{x}) x_{i} \\
w \sum_{i=1}^{m} x_{i}^{2}=\sum_{i=1}^{m} y_{i} x_{i}-\bar{y} \sum_{i=1}^{m} x_{i}+w \bar{x} \sum_{i=1}^{m} x_{i} \\
w \sum_{i=1}^{m} x_{i}^{2}-w \bar{x} \sum_{i=1}^{m} x_{i}=\sum_{i=1}^{m} y_{i} x_{i}-\bar{y} \sum_{i=1}^{m} x_{i} \\
w\left(\sum_{i=1}^{m} x_{i}^{2}-\bar{x} \sum_{i=1}^{m} x_{i}\right)=\sum_{i=1}^{m} y_{i} x_{i}-\bar{y} \sum_{i=1}^{m} x_{i}</script><p>整理得：</p>
<script type="math/tex; mode=display">
w=\frac{\sum_{i=1}^{m} y_{i} x_{i}-\bar{y} \sum_{i=1}^{m} x_{i}}{\sum_{i=1}^{m} x_{i}^{2}-\bar{x} \sum_{i=1}^{m} x_{i}}</script><p>其中：</p>
<script type="math/tex; mode=display">
\bar{y} \sum_{i=1}^{m} x_{i}=\frac{1}{m} \sum_{i=1}^{m} y_{i} \sum_{i=1}^{m} x_{i}=\bar{x} \sum_{i=1}^{m} y_{i}</script><script type="math/tex; mode=display">
\bar{x} \sum_{i=1}^{m} x_{i}=\frac{1}{m} \sum_{i=1}^{m} x_{i} \sum_{i=1}^{m} x_{i}=\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}</script><script type="math/tex; mode=display">
w=\frac{\sum_{i=1}^{m} y_{i} x_{i}-\bar{x} \sum_{i=1}^{m} y_{i}}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}}</script><h3 id="1-3将w向量化"><a href="#1-3将w向量化" class="headerlink" title="1.3将w向量化"></a>1.3将w向量化</h3><p>将$\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}=\bar{x} \sum_{i=1}^{m} x_{i}=\sum_{i=1}^{m} x_{i} \bar{x}$代入分母可以得到：</p>
<script type="math/tex; mode=display">
w=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m} x_{i} \bar{x}}=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \bar{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \bar{x}\right)}</script><p>由于：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{m} y_{i} \bar{x}=\bar{x} \sum_{i=1}^{m} y_{i}=\frac{1}{m} \sum_{i=1}^{m} x_{i} \sum_{i=1}^{m} y_{i}=\sum_{i=1}^{m} x_{i} \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} y_{i}=\sum_{i=1}^{m} x_{i} \bar{y} \\
\sum_{i=1}^{m} y_{i} \bar{x}=\bar{x} \sum_{i=1}^{m} y_{i}=\bar{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} y_{i}=m \bar{x} \bar{y}=\sum_{i=1}^{m} \bar{x} \bar{y} \\
\sum_{i=1}^{m} x_{i} \bar{x}=\bar{x} \sum_{i=1}^{m} x_{i}=\bar{x} \cdot m \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} x_{i}=m \bar{x}^{2}=\sum_{i=1}^{m} \bar{x}^{2}</script><p>可以得到：</p>
<script type="math/tex; mode=display">
w=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \bar{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \bar{x}\right)}=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \bar{x}-y_{i} \bar{x}+y_{i} \bar{x}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \bar{x}-x_{i} \bar{x}+x_{i} \bar{x}\right)}\\
=\frac{\sum_{i=1}^{m}\left(y_{i} x_{i}-y_{i} \bar{x}-x_{i} \bar{y}+\bar{x} \bar{y}\right)}{\sum_{i=1}^{m}\left(x_{i}^{2}-x_{i} \bar{x}-x_{i} \bar{x}+\bar{x}^{2}\right)}=\frac{\sum_{i=1}^{m}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{m}\left(x_{i}-\bar{x}\right)^{2}}</script><p>最终向量化有利于使用numpy库：</p>
<script type="math/tex; mode=display">
\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{m}\right)^{T} \quad \boldsymbol{y}=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{T} \\
\boldsymbol{x}_{d}=\left(x_{1}-\bar{x}, x_{2}-\bar{x}, \ldots, x_{m}-\bar{x}\right)^{T} \boldsymbol{y}_{d}=\left(y_{1}-\bar{y}, y_{2}-\bar{y}, \ldots, y_{m}-\bar{y}\right)^{T} \\
w=\frac{\sum_{i=1}^{m}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{m}\left(x_{i}-\bar{x}\right)^{2}} \\
=\frac{\boldsymbol{x}_{d}^{T} \boldsymbol{y}_{d}}{\boldsymbol{x}_{d}^{T} \boldsymbol{x}_{d}}</script><h2 id="2-多元线性回归"><a href="#2-多元线性回归" class="headerlink" title="2.多元线性回归"></a>2.多元线性回归</h2><ul>
<li>==由最小二乘法导出损失函数$E_{\widehat{w} }$==</li>
</ul>
<p>将$\mathbf{w}$和$b$组合成<script type="math/tex">\mathbf{\widehat{w} }</script>：</p>
<script type="math/tex; mode=display">
\begin{array}{r}
f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b \\
f\left(\boldsymbol{x}_{i}\right)=\left(\begin{array}{cccc}
w_{1} & w_{2} & \cdots & w_{d}
\end{array}\right)\left(\begin{array}{c}
x_{i 1} \\
x_{i 2} \\
\vdots \\
x_{i d}
\end{array}\right)+b \\
f\left(\boldsymbol{x}_{i}\right)=w_{1} x_{i 1}+w_{2} x_{i 2}+\ldots+w_{d} x_{i d}+b
\end{array}</script><p>将b改变为$w_{d+1}$</p>
<script type="math/tex; mode=display">
\begin{array}{c}
f\left(\boldsymbol{x}_{i}\right)=w_{1} x_{i 1}+w_{2} x_{i 2}+\ldots+w_{d} x_{i d}+w_{d+1} \cdot 1 \\
f\left(\boldsymbol{x}_{i}\right)=\left(\begin{array}{cccc}
w_{1} & w_{2} & \cdots & w_{d} &w_{d+1}
\end{array}\right) & \left(\begin{array}{r}
x_{i 1} \\
x_{i 2} \\
\vdots \\
x_{i d} \\
1
\end{array}\right) \\
f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}
\end{array}</script><script type="math/tex; mode=display">
由最小二乘法导出损失函数  E_{\dot{w}} \\

\begin{aligned}
E_{\hat{\boldsymbol{w}}} &=\sum_{i=1}^{m}\left(f\left(\hat{\boldsymbol{x}}_{i}\right)-y_{i}\right)^{2} \\
&=\sum_{i=1}^{m}\left(y_{i}-f\left(\hat{\boldsymbol{x}}_{i}\right)\right)^{2} \\
&=\sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}\right)^{2}
\end{aligned}</script><p>向量化：</p>
<script type="math/tex; mode=display">
\mathbf{X}=\left(\begin{array}{ccccc}
x_{11} & x_{12} & \ldots & x_{1 d} & 1 \\
x_{21} & x_{22} & \ldots & x_{2 d} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m 1} & x_{m 2} & \ldots & x_{m d} & 1
\end{array}\right)=\left(\begin{array}{cc}
\boldsymbol{x}_{1}^{\mathrm{T}} & 1 \\
\boldsymbol{x}_{2}^{\mathrm{T}} & 1 \\
\vdots & \vdots \\
\boldsymbol{x}_{m}^{\mathrm{T}} & 1
\end{array}\right)=\left(\begin{array}{c}
\hat{\boldsymbol{x}}_{1}^{T} \\
\hat{\boldsymbol{x}}_{2}^{T} \\
\vdots \\
\hat{\boldsymbol{x}}_{m}^{T}
\end{array}\right) \\
\boldsymbol{y}=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{T} \\
E_{\hat{\boldsymbol{w}}}=\sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{i}\right)^{2} \\
 =\left(y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1}\right)^{2}+\left(y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2}\right)^{2}+\ldots+\left(y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}\right)^{2}</script><script type="math/tex; mode=display">
E_{\hat{\boldsymbol{w}}}=\left(y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \cdots y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}\right)\left(\begin{array}{c}
y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} \\
y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \\
\vdots \\
y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}
\end{array}\right)</script><p>又因为：（原理：标量相乘可以添加转置）</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}
y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} \\
y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \\
\vdots \\
y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}
\end{array}\right)=\left(\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{m}
\end{array}\right)-\left(\begin{array}{c}
\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} \\
\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \\
\vdots \\
\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}
\end{array}\right)=\left(\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{m}
\end{array}\right)-\left(\begin{array}{c}
\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}} \\
\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}} \\
\vdots \\
\hat{\boldsymbol{x}}_{m}^{T} \hat{\boldsymbol{w}}
\end{array}\right)</script><script type="math/tex; mode=display">
\left(\begin{array}{c}
\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}} \\
\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}} \\
\vdots \\
\hat{\boldsymbol{x}}_{m}^{T} \hat{\boldsymbol{w}}
\end{array}\right)=\left(\begin{array}{c}
\hat{\boldsymbol{x}}_{1}^{T} \\
\hat{\boldsymbol{x}}_{2}^{T} \\
\vdots \\
\hat{\boldsymbol{x}}_{m}^{T}
\end{array}\right) \text { . } \hat{\boldsymbol{w}}=\mathbf{X} \boldsymbol{\hat { w }}</script><p>所以：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
\left(\begin{array}{c}
y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} \\
y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \\
\vdots \\
y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}
\end{array}\right)=\left(\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{m}
\end{array}\right)-\left(\begin{array}{c}
\hat{\boldsymbol{x}}_{1}^{T} \hat{\boldsymbol{w}} \\
\hat{\boldsymbol{x}}_{2}^{T} \hat{\boldsymbol{w}} \\
\vdots \\
\hat{\boldsymbol{x}}_{m}^{T} \hat{\boldsymbol{w}}
\end{array}\right)=\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}} \\
E_{\hat{\boldsymbol{w}}}=\left(y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \cdots y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}\right)\left(\begin{array}{c}

y_{1}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{1} \\
y_{2}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{2} \\
\vdots \\
y_{m}-\hat{\boldsymbol{w}}^{T} \hat{\boldsymbol{x}}_{m}
\end{array}\right)
\end{array}</script><script type="math/tex; mode=display">
=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}){\text {此即为式 } 3.9} \text { argmin后面的部分 }</script><ul>
<li>==证明损失函数$E_{\widehat{w} }$是关于$\mathbf{\widehat{w} } $的凸函数==</li>
</ul>
<p><strong>凸集</strong>：设集合$D\in R^n$，如果对于任意的$x,y\in D$与任意的$a\in [0,1]$，有$\alpha x+(1-\alpha)y\in D$，则称集合是凸集</p>
<p>凸集的集合意义：若两个点属于此集合，则这两个点连线上的任意一点都属于此集合</p>
<script type="math/tex; mode=display">
梯度定义：设n元函数  f(x)  对自变量  \boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T} \quad  的各分量  x_{i} 
的偏导数  \frac{\partial f(\boldsymbol{x})}{\partial x_{i}} \quad(i=1,2, \ldots, n) \quad  \\都存在，则称函数  f(\boldsymbol{x})  在  \boldsymbol{x}  处一阶可导
\\并称向量

\nabla f(\boldsymbol{x})=\left(\begin{array}{c}
\frac{\partial f(\boldsymbol{x})}{\partial x_{1}} \\
\frac{\partial f(\boldsymbol{x})}{\partial x_{2}} \\
\vdots \\
\frac{\partial f(\boldsymbol{x})}{\partial x_{n}}
\end{array}\right)

函数  f(\boldsymbol{x})  在  \boldsymbol{x}  处的一阶导数或梯度，记为  \nabla f(\boldsymbol{x}) \quad(  列向量  )</script><p><strong>海塞矩阵</strong>：设n元函数$f(x)$对自变量$\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T}0$的各分量$\boldsymbol{x_i}$的二阶偏导数$\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{j}} \quad\left(i=1,2, \ldots, n_{;}, j=1,2, \ldots, n\right)$都存在，则称函数$f(x)$在点$\boldsymbol{x}$处二阶可导，并称矩阵：</p>
<script type="math/tex; mode=display">
\nabla^{2} f(\boldsymbol{x})=\left[\begin{array}{cccc}
\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1}^{2}} & \frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1} \partial x_{2}} & \cdots & \frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{1} \partial x_{n}} \\
\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2} \partial x_{1}} & \frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2}^{2}} & \cdots & \frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{2} \partial x_{n}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n} \partial x_{1}} & \frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n} \partial x_{2}} & \cdots & \frac{\partial^{2} f(\boldsymbol{x})}{\partial x_{n}^{2}}
\end{array}\right]</script><p>为函数$f(x)$在$\boldsymbol{x}$处的二阶导数或Hessian矩阵，记为$\nabla^{2} f(\boldsymbol{x})$，若$f(x)$对$\boldsymbol{x}$各元变量的所有二阶偏导数都连续，则$\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{j}}=\frac{\partial^{2} f(x)}{\partial x_{j} \partial x_{i}}$，此时$\nabla^{2} f(\boldsymbol{x})$为对称矩阵。</p>
<p><strong>多元实值函数凸凹性判定定理</strong>：</p>
<p>设$D\subset R^n$是非空开凸集，$f:D\subset R^n \to R$,且$f(x)$在$D$上二阶连续可微，如果$f(x)$的Hessian矩阵$\nabla^{2} f(\boldsymbol{x})$在$D$上是正定的，则$f(x)$是$D$上严格的凸函数。</p>
<p><strong>凸充分性定理</strong></p>
<p>若$f:R^n \to R$是凸函数，且$f(x)$一阶连续可微，则$\boldsymbol{x}^<em>$是全局解的充分必要条件是$\nabla f(\boldsymbol{x}^</em>)=\boldsymbol{0}$，其中$\nabla f(\boldsymbol{x})$为$f(x)$关于$\boldsymbol{x}$的一阶导数</p>
<p><strong>【标量—向量】的矩阵微分公式</strong></p>
<p>其中$\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{T}$为n维列向量，$y$为$\boldsymbol{x}$的n元标量函数</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \boldsymbol{x}}=\left(\begin{array}{c}
\frac{\partial y}{\partial x_{1}} \\
\frac{\partial y}{\partial x_{2}} \\
\vdots \\
\frac{\partial y}{\partial x_{n}}
\end{array}\right) \quad \begin{array}{c}
 \\
\frac{\partial y}{\partial \boldsymbol{x}}=\left(\begin{array}{cccc}
\frac{\partial y}{\partial x_{1}} & \frac{\partial y}{\partial x_{2}} & \cdots & \frac{\partial y}{\partial x_{n}}
\end{array}\right)
\end{array}</script><p>左侧为分母布局（默认采用），右侧为分子布局</p>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{x}^{T} \boldsymbol{a}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{a}^{T} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\begin{array}{c}
\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{1}} \\
\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{2}} \\
\vdots \\
\frac{\partial\left(a_{1} x_{1}+a_{2} x_{2}+\ldots+a_{n} x_{n}\right)}{\partial x_{n}}
\end{array}\right)=\left(\begin{array}{c}
a_{1} \\
a_{2} \\
\vdots \\
a_{n}
\end{array}\right)=\boldsymbol{a}</script><p>同理可以推出：</p>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{x}^{T} \mathbf{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{B}+\mathbf{B}^{T}\right) \boldsymbol{x}</script><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}} &=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{T}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\right] \\
&=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[\left(\boldsymbol{y}^{T}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T}\right)(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})\right] \\
&=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[\boldsymbol{y}^{T} \boldsymbol{y}-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}\right] \\
&=\frac{\partial}{\partial \hat{\boldsymbol{w}}}\left[-\boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}+\hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}\right]\\
&=-\frac{\partial \boldsymbol{y}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{u}}}-\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \boldsymbol{y}}{\partial \hat{\boldsymbol{w}}}+\frac{\partial \hat{\boldsymbol{w}}^{T} \mathbf{X}^{T} \mathbf{X} \hat{\boldsymbol{w}}}{\partial \hat{\boldsymbol{w}}}
\end{aligned}</script><p>由矩阵微分公式$\frac{\partial \boldsymbol{x}^{T} \boldsymbol{a}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{a}^{T} \boldsymbol{x}}{\partial \dot{\boldsymbol{x}}}=\boldsymbol{a} \quad \frac{\partial \boldsymbol{x}^{T} \mathbf{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{B}+\mathbf{B}^{T}\right) \boldsymbol{x}$可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=-\mathbf{X}^{T} \boldsymbol{y}-\mathbf{X}^{T} \boldsymbol{y}+\left(\mathbf{X}^{T} \mathbf{X}+\mathbf{X}^{T} \mathbf{X}\right) \hat{w}\\
&=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y}) \quad \text { 此即为式3.10 }
\end{aligned}</script><p>求二阶导数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial^{2} E_{\hat{w}}}{\partial \hat{w} \partial \hat{w}^{T}} &=\frac{\partial}{\partial \hat{w}}\left(\frac{\partial E_{\hat{w}}}{\partial \hat{w}}\right) \\
&=\frac{\partial}{\partial \hat{w}}\left[2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y})\right] \\
&=\frac{\partial}{\partial \hat{w}}\left(2 \mathbf{X}^{T} \mathbf{X} \hat{w}-2 \mathbf{X}^{T} \boldsymbol{y}\right) \\
&=2 \mathbf{X}^{T} \mathbf{X} \quad \text { 此即为Hessian矩阵 }
\end{aligned}</script><p>西瓜书上假设$\mathbf{X}^{T} \mathbf{X}$为正定矩阵，则保证了凸凹性</p>
<ul>
<li>==对损失函数$E_{\widehat{w} }$关于$\mathbf{\widehat{w} } $求一阶导数==</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y})</script><ul>
<li>==令一阶导数等于<strong>0</strong>求解出$\widehat{w} ^*$==</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2 \mathbf{X}^{T}(\mathbf{X} \hat{w}-\boldsymbol{y})=0 \\
2 \mathbf{X}^{T} \mathbf{X} \hat{w}-2 \mathbf{X}^{T} \boldsymbol{y}=0 \\
2 \mathbf{X}^{T} \mathbf{X} \hat{w}=2 \mathbf{X}^{T} \boldsymbol{y}
\\
\hat{w}=\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \boldsymbol{y} \quad \text { 此即为式3.11 }</script><h2 id="3-对数几率回归"><a href="#3-对数几率回归" class="headerlink" title="3.对数几率回归"></a>3.对数几率回归</h2><h3 id="3-1广义线性模型"><a href="#3-1广义线性模型" class="headerlink" title="3.1广义线性模型"></a>3.1广义线性模型</h3><p><strong>指数族分布</strong></p>
<p>指数族（Exponential family）分布是一类分布的总称，该类分布的分布律（或者概率密</p>
<p>度函数）的一般形式如下：</p>
<script type="math/tex; mode=display">
p(y ; \eta)=b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right)</script><p>其中$\eta$称为该分布的自然参数，$T(y)$为充分统计量，视具体的分布而定，通常是等于随机变量$y$本身，$a(\eta)$为分配函数，$b(y)$为关于随机变量$y$的函数，常见的伯努利分布和正泰分布都是指数族分布。</p>
<p>证明伯努利分布是指数族分布：</p>
<p>已知伯努利分布的分布律为：</p>
<script type="math/tex; mode=display">
p(y)=\phi^{y}(1-\phi)^{1-y}</script><p>其中$y\in \{0,1 \}$，$\phi$为$y=1$的概率，即：$p(y=1)=\phi$，对上式恒等变形可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y) &=\phi^{y}(1-\phi)^{1-y} \\
&=\exp \left(\ln \left(\phi^{y}(1-\phi)^{1-y}\right)\right) \\
&=\exp \left(\ln \phi^{y}+\ln (1-\phi)^{1-y}\right)\\
&=\exp (y \ln \phi+(1-y) \ln (1-\phi)) \\
&=\exp (y \ln \phi+\ln (1-\phi)-y \ln (1-\phi)) \\
&=\exp (y(\ln \phi-\ln (1-\phi))+\ln (1-\phi)) \\
&=\exp \left(y \ln \left(\frac{\phi}{1-\phi}\right)+\ln (1-\phi)\right)
\end{aligned}</script><p><strong>广义线性模型三条假设</strong></p>
<ul>
<li>在给定$\boldsymbol{x}$的条件下，假设随机变量$y$服从某个指数族分布</li>
<li>在给定$\boldsymbol{x}$的条件下，我们的目标是得到一个模型$h(x)$能预测出$T(y)$的期望值</li>
<li>假设该指数族分布中的自然参数$\eta$与$\boldsymbol{x}$成线性关系，即：$\eta=\boldsymbol{w}^{T} \boldsymbol{x}$</li>
</ul>
<p><strong>对数几率回归的广义线性模型推导</strong></p>
<p>对数几率回归是在对一个二分类问题进行建模，并且假设被建模的随机变量<script type="math/tex">y</script>取值为0或1，因此我们可以很自然得假设<script type="math/tex">y</script>服从伯努利分布。此时，如果我们想要构建一个线性模型来预测在给定$\boldsymbol{x}$的条件下<script type="math/tex">y</script>的取值的话，可以考虑使用广义线性模型来进行建模。</p>
<p>已知y是服从伯努利分布，而伯努利分布属于指数族分布，所以满足广义线性模型的第一条假设，接着根据广义线性模型的第二条假设我们可以推得模型$h\boldsymbol{(x)}$的表达式为：</p>
<script type="math/tex; mode=display">
h(\boldsymbol{x})=E[T(y \mid \boldsymbol{x})]</script><p>由于伯努利分布的$T(y \mid \boldsymbol{x})=y \mid \boldsymbol{x}$，所以：</p>
<script type="math/tex; mode=display">
h(\boldsymbol{x})=E[y \mid \boldsymbol{x}]</script><p>又因为$E[y \mid \boldsymbol{x}]=1 \times p(y=1 \mid \boldsymbol{x})+0 \times p(y=0 \mid \boldsymbol{x})=p(y=1 \mid \boldsymbol{x})=\phi$，所以：</p>
<script type="math/tex; mode=display">
h(\boldsymbol{x})=\phi</script><p>在前面证明伯努利分布属于指数族分布时我们知道：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
\eta=\ln \left(\frac{\phi}{1-\phi}\right) \\
e^{\eta}=\frac{\phi}{1-\phi} \\
e^{-\eta}=\frac{1-\phi}{\phi} \\
e^{-\eta}=\frac{1}{\phi}-1 \\
1+e^{-\eta}=\frac{1}{\phi} \\
\frac{1}{1+e^{-\eta}}=\phi
\end{array}</script><p>将$\frac{1}{1+e^{-\eta}}=\phi$代入$h(\boldsymbol{x})$的表达式可以得到：</p>
<script type="math/tex; mode=display">
h(\boldsymbol{x})=\phi=\frac{1}{1+e^{-\eta}}</script><p>根据广义模型的第三条假设：$\eta=\boldsymbol{w}^{T} \boldsymbol{x}$，<script type="math/tex">h(\boldsymbol{x})</script>最终可以表示为：</p>
<script type="math/tex; mode=display">
h(\boldsymbol{x})=\phi=\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}}=p(y=1 \mid \boldsymbol{x}) \quad \text { 此即为式3.23 }</script><h3 id="3-2极大似然估计法"><a href="#3-2极大似然估计法" class="headerlink" title="3.2极大似然估计法"></a>3.2极大似然估计法</h3><p>极大似然估计法：</p>
<p>设总体的概率密度函数（或分布律）为 $f\left(y, w_{1}, w_{2}, \ldots, w_{k}\right)$, ${y}_{1}, y_{2}, \ldots, y_{m}$为从该总体中抽出的样本。因为${y}_{1}, y_{2}, \ldots, y_{m}$相互独立且同分布，于是，它们的联合概率密度函数（或联合概率）为：</p>
<script type="math/tex; mode=display">
L\left(y_{1}, y_{2}, \ldots, y_{m} ; w_{1}, w_{2}, \ldots, w_{k}\right)=\prod_{i=1}^{m} f\left(y_{i}, w_{1}, w_{2}, \ldots, w_{k}\right)</script><p>其中$w_{1}, w_{2}, \ldots, w_{k}$被看做固定但是未知的参数。当我们已经观测到一组样本观测值${y}_{1}, y_{2}, \ldots, y_{m}$时，要去估计未知参数，一种直观的想法就是，哪一组参数值使得现在的样本观测值出现的概率最大，哪一组就是真正的参数，我们就用它作为参数的估测值，这就是作为的极大似然估计。</p>
<p><strong>极大似然估计的具体方法</strong></p>
<p>通常记${L}\left(y_{1}, y_{2}, \ldots, y_{m} ; w_{1}, w_{2}, \ldots, w_{k}\right)={L}(w)$，并称其为似然函数，于是，求$w$的极大似然估计就归结为求$L(w)$的最大值点，由于对数函数是单调递增函数，所以：</p>
<script type="math/tex; mode=display">
\ln L(\boldsymbol{w})=\ln \left(\prod_{i=1}^{m} f\left(y_{i}, w_{1}, w_{2}, \ldots, w_{k}\right)\right)=\sum_{i=1}^{m} \ln f\left(y_{i}, w_{1}, w_{2}, \ldots, w_{k}\right)</script><p>与$L(w)$有相同的极大值点，而在许多情况下，求$\ln L(w)$的最大值点比较简单，于是我们就将求$L(w)$的最大值点转化为了求$\ln L(w)$的最大值点，通常称$\ln L(w)$为对数似然函数。</p>
<p><strong>对数几率回归的极大似然估计</strong></p>
<p>已知随机变量y取1和0的概率分别为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
p(y=1 \mid \boldsymbol{x})=\frac{e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}} \\
p(y=0 \mid \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}
\end{array}</script><p>令$\beta =(w;b),\hat{x}=(x;1)$，则$\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b$可以简写为$\beta^T \hat{x}$，于是上面的式子可以化简为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
p(y=1 \mid \boldsymbol{x})=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}} \\
p(y=0 \mid \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}
\end{array}</script><p>记：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
p(y=1 \mid \boldsymbol{x})=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}=p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta}) \\
p(y=0 \mid \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}}}=p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})
\end{array}</script><p>于是，使用一个小技巧即可得到随机变量y的分布律表达式：</p>
<script type="math/tex; mode=display">
p(y \mid \boldsymbol{x} ; \boldsymbol{w}, b)=y \cdot p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})+(1-y) \cdot p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta}) \quad  此即为式 3.26
\\ 或者

p(y \mid \boldsymbol{x} ; \boldsymbol{w}, b)=\left[p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{y}\left[p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{1-y}</script><h3 id="3-3对数几率回归的参数估计"><a href="#3-3对数几率回归的参数估计" class="headerlink" title="3.3对数几率回归的参数估计"></a>3.3对数几率回归的参数估计</h3><p>根据对数似然函数的定义可以知道：</p>
<script type="math/tex; mode=display">
\ln L(\boldsymbol{w})=\sum_{i=1}^{m} \ln f\left(y_{i}, w_{1}, w_{2}, \ldots, w_{k}\right)</script><p>由于此时的y为离散型，所以将对数似然函数中的概率密度函数换成分布律即可：</p>
<script type="math/tex; mode=display">
\ell(\boldsymbol{w}, b):=\ln L(\boldsymbol{w}, b)=\sum_{i=1}^{m} \ln p\left(y_{i} \mid \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right) \quad \text { 此即为式3.25 }</script><p>将$p(y \mid \boldsymbol{x} ; \boldsymbol{w}, b)=y \cdot p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})+(1-y) \cdot p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})$代入对数似然函数可得：</p>
<script type="math/tex; mode=display">
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m} \ln \left(y_{i} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)+\left(1-y_{i}\right) p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)</script><p>由于$p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}} \quad, \quad p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}$，上面的式子可以化为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\ell(\boldsymbol{\beta}) &=\sum_{i=1}^{m} \ln \left(\frac{y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}+\frac{1-y_{i}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right) \\
&=\sum_{i=1}^{m} \ln \left(\frac{y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-y_{i}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right) \\
&=\sum_{i=1}^{m}\left(\ln \left(y_{i} e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-y_{i}\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)
\end{aligned}</script><p>由于$y_i \in \{ 0,1\}$，所以：</p>
<p>当$y_i=0$时：</p>
<script type="math/tex; mode=display">
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(\ln \left(0 \cdot e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-0\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)=\sum_{i=1}^{m}\left(\ln 1-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)\\=\sum_{i=1}^{m}\left(-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)</script><p>当$y_i=1$时：</p>
<script type="math/tex; mode=display">
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(\ln \left(1 \cdot e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}+1-1\right)-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)\\=\sum_{i=1}^{m}\left(\ln e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)=\sum_{i=1}^{m}\left(\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)</script><p>综合可得：</p>
<script type="math/tex; mode=display">
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(y_{i} \boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right)</script><p><strong>另一种形式的参数估计：</strong></p>
<p>若$p(y \mid \boldsymbol{x} ; \boldsymbol{w}, b)=\left[p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{y}\left[p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})\right]^{1-y}$，将其代入对数似然函数可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\ell(\boldsymbol{\beta}) &=\sum_{i=1}^{m} \ln \left(\left[p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{y_{i}}\left[p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{1-y_{i}}\right) \\
&=\sum_{i=1}^{m}\left[\ln \left(\left[p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{y_{i}}\right)+\ln \left(\left[p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right]^{1-y_{i}}\right)\right] \\
&=\sum_{i=1}^{m}\left[y_{i} \ln \left(p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)+\left(1-y_{i}\right) \ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right] \\
&=\sum_{i=1}^{m}\left\{y_{i}\left[\ln \left(p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)-\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right]+\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right\}
\end{aligned}</script><script type="math/tex; mode=display">
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left[y_{i} \ln \left(\frac{p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)}{p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)}\right)+\ln \left(p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)\right]</script><p>由于$p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}} \quad, \quad p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)=\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left[y_{i} \ln \left(e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)+\ln \left(\frac{1}{1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}}\right)\right]\\
&=\sum_{i=1}^{m}\left(y_{i} \boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{T} \hat{\boldsymbol{x}}_{i}}\right)\right) \quad \text { 加个负号即为式 } 3.27
\end{aligned}</script><h2 id="4-线性判别分析"><a href="#4-线性判别分析" class="headerlink" title="4.线性判别分析"></a>4.线性判别分析</h2><p>Linear Discriminant Analysis</p>
<p>用途：数据预处理中的降维，分类任务</p>
<p>历史：Ronald A. Fisher在1936年提出了线性判别方法</p>
<p>目标：LDA关心的是能够最大化类间区分度的坐标轴成分，将特征空间（数据集中的多维样本）投影到一个维度更小的 k 维子空间中，同时保持区分类别的信息</p>
<p>​        LDA作为特征降维工作的原理是，根据带标签的数据点，通过投影的办法，投影到维度更低的空间中，使得投影后的点，按照类别进行划分，同一类别的数据点，在投影后的空间中更接近。</p>
<p><strong>直观理解</strong></p>
<p>计算每个类别数据的中心：</p>
<script type="math/tex; mode=display">
m_1=\frac{1}{N_1}\sum_{n\in C_1}x_n\ \ \ \ \ \ m_2=\frac{1}{N_2}\sum_{n\in C_2}x_n</script><p>在降维过程中我们将数据都在这条连线上投影，但是我们发现，如果将数据直接投影到连接两个中心的向量W上降维，数据有重叠，并不是最优，本来可以进行线性分割数据，降维之后不能分割了。</p>
<p>降维的数据满足两个特征来减少重叠：</p>
<ul>
<li>不同类别的数据将为后相互间的差异大</li>
<li>同一类别数据降维后相互间差异小</li>
</ul>
<p>也就是说：最大化类间距离和最小化类内距离</p>
<p><strong>LDA数学推导</strong></p>
<p>我们以最简单的二分类为切入口，首先我们定义两个样本$C_1,C_2$，均值分别为$\mu_1,\mu_2$，投影方向为$\omega$，则投影后两个样本之间的距离就可以表示为：</p>
<script type="math/tex; mode=display">
D(C_1,C_2)=||\omega^T(\mu_1-\mu_2)||_2^T</script><p>接着我们需要表示出投影后的样本方差：</p>
<script type="math/tex; mode=display">
Var(C_1')=\sum_{x\in C_1}(\omega^Tx-\omega^T\mu_1)^2\\
Var(C_2')=\sum_{x\in C_2}(\omega^Tx-\omega^T\mu_2)^2</script><p>此时我们的优化目标可以写为：</p>
<script type="math/tex; mode=display">
J ( w ) = \frac { D ( C_ 1 , C_ 2 ) } { \operatorname { Var } \left( C _ { 1 } \right) + \operatorname { Var } \left( C _ { 2 } ^ { \prime } \right) }</script><p>带入上式化简后我们可得：</p>
<script type="math/tex; mode=display">
J ( w ) = \frac { w ^ { T } \left( \mu _ { 1 } - \mu _ { 2 } \right) \left( \mu _ { 1 } - \mu _ { 2 } \right) ^ { T } w } { \sum _ { x \in C _ { i } } w ^ { T } \left( x - \mu _ { i } \right) \left( x - \mu _ { i } \right) ^ { T } w }</script><p>此时我们可以分别定义类间散度矩阵$S_b$，以及类内散度矩阵$S_w$如下：</p>
<script type="math/tex; mode=display">
 S _ { w } = \sum _ { x \in C _ { i } } \left( x - \mu _ { i } \right) \left( x - \mu _ { i } \right) ^ { T } \\ S _ { b } = \left( \mu _ { 1 } - \mu _ { 2 } \right) \left( \mu _ { 1 } - \mu _ { 2 } \right) ^ { T }</script><p>则此时我们的优化目标可以简化为：</p>
<script type="math/tex; mode=display">
\max J ( w ) = \frac { w ^ { T } S _ { b } w } { w ^ { T } S _ { w } w }</script><p>我们要最大化$J(\omega)$只需要对$\omega$求偏导，并令导数=0，得到：</p>
<script type="math/tex; mode=display">
\left( w ^ { T } S _ { w } w \right) S _ { b } w = \left( w ^ { T } S _ { b } w \right) S _ { w } w</script><p>带入原式，我们便可以得到：</p>
<script type="math/tex; mode=display">
S_b\omega=\lambda S_{\omega}\omega</script><p>其中$\lambda$为$J(\omega)$，显然为一个数，那么我们再整理一下，我们就可以得到：</p>
<script type="math/tex; mode=display">
S_{\omega}^{-1}S_b\omega=\lambda\omega</script><p>至此，我们将最大化的目标就对应了矩阵的最大特征值，而投影方向就是这个特征值对应的特征向量。</p>
<p>可将LDA从二分类扩展至多分类高维情况。</p>
<p><strong>LDA的流程：</strong></p>
<p>（1）计算每个类别的均值为$\mu_i$，全局样本均值为$\mu$</p>
<p>（2）计算类内散度矩阵$S_\omega$，全局散度矩阵为$S_t$，类间散度矩阵为$S_b$</p>
<p>（3）对矩阵$S_{\omega}^{-1}S_b$进行特征值分解</p>
<p>（4）取最大的$d’$个特征值所对应的特征向量</p>
<p>（5）计算投影矩阵</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>buptyqx</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://example.com/2022/06/27/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">http://example.com/2022/06/27/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2022/06/27/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B01/">算法笔记1</a>
            
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© buptyqx | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>